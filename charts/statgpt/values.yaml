# -- Backend version is used for both chat-backend and admin-backend image tags (must be the same for both)
_backend_version: &backend_version "0.4.0"
# -- Admin Frontend version is used for the admin-frontend image tag
_admin_frontend_version: &admin_frontend_version "0.3.0"
# -- Portal Frontend version is used for the portal-frontend image tag
_portal-frontend_version: &portal-frontend_version "0.2.0"
# -- PostgreSQL version is used for the postgresql image tag
_postgresql_version: &postgresql_version "16.3.0-debian-12-r14"
# -- PGVector extension version
_pgvector_version: &pgvector_version "v0.8.1"
# -- Elasticsearch version is used for the elasticsearch image tag
_elasticsearch_version: &elasticsearch_version "8.14.3-debian-12-r0"

chat-backend:
  # -- Indicates whether the chat-backend service is enabled
  enabled: false

  ### Pods Security Context Configuration ###
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  podSecurityContext:
    # -- Enable security context for the pod
    enabled: true
    # -- User ID under which the pod runs
    runAsUser: 5678

  commonLabels:
    # -- Kubernetes label to identify the component as an application
    app.kubernetes.io/component: "application"

  ### Container Image Configuration ###
  # ref: https://kubernetes.io/docs/concepts/containers/images/
  image:
    # -- Docker registry URL
    registry: docker.io
    # -- Image repository name
    repository: epam/statgpt-chat-backend
    # -- Image tag or version
    tag: *backend_version
    # -- Image pull policy
    pullPolicy: Always

  ### Container Port Configuration ###
  # ref: https://kubernetes.io/docs/concepts/services-networking/service/
  containerPorts:
    # -- HTTP port for the application
    http: 5000

  ### Liveness Probes configuration ###
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  livenessProbe:
    # -- Enable livenessProbe
    enabled: true
    # -- Initial delay in seconds before liveness probe starts (increased to prevent premature pod restarts during PostgreSQL initialization)
    initialDelaySeconds: 180

  ### Readiness Probes configuration ###
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  readinessProbe:
    # -- Enable readinessProbe
    enabled: true

  ### Resource Management ###
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  resources:
    limits:
      # -- Maximum CPU limit for the container
      cpu: "1000m"
      # -- Maximum memory limit for the container
      memory: "4Gi"
    requests:
      # -- Minimum CPU request for resource scheduling
      cpu: "100m"
      # -- Minimum memory request for resource scheduling
      memory: "2Gi"

  metrics:
    # -- Enable metrics collection
    enabled: false
    serviceMonitor:
      # -- Enable Prometheus ServiceMonitor for metrics
      enabled: false

  ### Environment Variables ###
  env:
    # -- Name of the DIAL app for OpenTelemetry
    DIAL_APP_NAME: "StatGPT"
    # -- Show debug stages in DIAL
    DIAL_SHOW_DEBUG_STAGES: "false"
    # -- Show stage seconds in DIAL
    DIAL_SHOW_STAGE_SECONDS: "false"
    # -- Number of concurrent web processes
    WEB_CONCURRENCY: "1"
    # -- Enable verbose logging for Langchain
    LANGCHAIN_VERBOSE: "false"
    # -- Enable debug logging for Langchain
    LANGCHAIN_DEBUG: "false"
    # -- Default seed for Langchain operations
    LANGCHAIN_DEFAULT_SEED: "820288"

    # -- URL for DIAL application
    DIAL_URL: "environment-specific"
    # -- Host for PGVector database
    PGVECTOR_HOST: "environment-specific"
    # -- Port for PGVector database
    PGVECTOR_PORT: "environment-specific"
    # -- Database name for PGVector
    PGVECTOR_DATABASE: "environment-specific"
    # -- Use Azure Managed Identity for PostgreSQL (set to "true" to use MSI instead of PGVECTOR_PASSWORD)
    PGVECTOR_USE_MSI: "false"
    # -- Connection string for Elasticsearch
    ELASTIC_CONNECTION_STRING: "environment-specific"
    # -- Index for Elasticsearch indicators
    ELASTIC_INDICATORS_INDEX: "environment-specific"
    # -- Index for Elasticsearch matching
    ELASTIC_MATCHING_INDEX: "environment-specific"

    ### Data Related Variables ###
    # Configure these variables based on the data uploaded to the application
    # Must be defined for both chat-backend and admin-backend
    # Example:
    # DATA_PORTAL_URL: "example"
    # Note: DATA_PORTAL_API_KEY should be configured in secrets section below

  ### Secrets Configuration ###
  # Sensitive values are stored in Kubernetes Secrets and mounted as environment variables
  # The secrets are automatically created and mounted by the dial-extension chart
  # ref: https://kubernetes.io/docs/concepts/configuration/secret/
  secrets: {}
    # Uncomment and configure the following secrets in your environment-specific values:
    #
    # -- DIAL API key for authentication
    # DIAL_API_KEY: ""
    # -- PGVector database username
    # PGVECTOR_USER: ""
    # -- PGVector database password
    # PGVECTOR_PASSWORD: ""
    # -- Elasticsearch authentication username
    # ELASTIC_AUTH_USER: ""
    # -- Elasticsearch authentication password
    # ELASTIC_AUTH_PASSWORD: ""

    ### Data Related Variables ###
    # -- Example for data related variables
    # DATA_PORTAL_API_KEY: "example"

admin-backend:
  # -- Indicates whether the admin-backend service is enabled
  enabled: false

  commonLabels:
    # -- Kubernetes label to identify the component as an application
    app.kubernetes.io/component: "application"

  ### Container Image Configuration ###
  # ref: https://kubernetes.io/docs/concepts/containers/images/
  image:
    # -- Docker registry URL
    registry: docker.io
    # -- Image repository name
    repository: epam/statgpt-admin-backend
    # -- Image tag or version
    tag: *backend_version
    # -- Image pull policy
    pullPolicy: Always

  ### Init Containers Configuration ###
  # ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
  initContainers:
    - name: alembic
      image: "{{ .Values.image.registry }}/{{ .Values.image.repository }}:{{ .Values.image.tag }}"
      imagePullPolicy: "{{ .Values.image.pullPolicy }}"
      env:
        - name: "ADMIN_MODE"
          value: "INIT"
        - name: "PGVECTOR_HOST"
          value: "{{ .Values.env.PGVECTOR_HOST}}"
        - name: "PGVECTOR_PORT"
          value: "{{ .Values.env.PGVECTOR_PORT }}"
        - name: "PGVECTOR_DATABASE"
          value: "{{ .Values.env.PGVECTOR_DATABASE }}"
        # When PGVECTOR_USE_MSI is "true", Managed Identity is used and PGVECTOR_PASSWORD is not required
        - name: "PGVECTOR_USE_MSI"
          value: "{{ .Values.env.PGVECTOR_USE_MSI }}"
        # Configuration via Kubernetes secrets is required
        - name: "PGVECTOR_USER"
          value: "{{ .Values.secrets.PGVECTOR_USER }}"
        - name: "PGVECTOR_PASSWORD"
          value: "{{ .Values.secrets.PGVECTOR_PASSWORD }}"

  ### Container Port Configuration ###
  # ref: https://kubernetes.io/docs/concepts/services-networking/service/
  containerPorts:
    # -- HTTP port for the application
    http: 8000

  ### Liveness Probes configuration ###
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  livenessProbe:
    # -- Enable livenessProbe
    enabled: true

  ### Readiness Probes configuration ###
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  readinessProbe:
    # -- Enable readinessProbe
    enabled: true

  ### Resource Management ###
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  resources:
    limits:
      # -- Maximum CPU limit for the container
      cpu: "1000m"
      # -- Maximum memory limit for the container
      memory: "6Gi"
    requests:
      # -- Minimum CPU request for resource scheduling
      cpu: "100m"
      # -- Minimum memory request for resource scheduling
      memory: "2Gi"

  metrics:
    # -- Enable metrics collection
    enabled: false
    serviceMonitor:
      # -- Enable Prometheus ServiceMonitor for metrics
      enabled: false

  ### Environment Variables ###
  env:
    # -- Application mode for admin settings
    ADMIN_MODE: "APP"
    # -- Number of concurrent web processes
    WEB_CONCURRENCY: "1"

    # -- Enable or disable OIDC authentication
    OIDC_AUTH_ENABLED: "true"
    # -- URL to fetch OIDC configuration (required if OIDC_AUTH_ENABLED="true")
    OIDC_CONFIGURATION_ENDPOINT: "environment-specific"
    # -- Client ID for OIDC authentication (required if OIDC_AUTH_ENABLED="true")
    OIDC_CLIENT_ID: "environment-specific"
    # -- OIDC issuer URL (required if OIDC_AUTH_ENABLED="true")
    OIDC_ISSUER: "environment-specific"
    # -- Specify claim used for the username extraction (required if OIDC_AUTH_ENABLED="true")
    OIDC_USERNAME_CLAIM: "environment-specific"
    # -- Claim used to identify user roles (required if OIDC_AUTH_ENABLED="true")
    ADMIN_ROLES_CLAIM: "environment-specific"
    # -- Values within the role claim that signify admin access (required if OIDC_AUTH_ENABLED="true")
    ADMIN_ROLES_VALUES: "environment-specific"

    # -- If "true", the admin portal will check for scopes in the OIDC token, otherwise this check will be skipped
    ADMIN_SCOPE_CLAIM_VALIDATION_ENABLED: "true"
    # -- Name of the access token field (required if OIDC_AUTH_ENABLED="true" and ADMIN_SCOPE_CLAIM_VALIDATION_ENABLED="true")
    ADMIN_SCOPE_CLAIM: "environment-specific"
    # -- Scope claim value (required if OIDC_AUTH_ENABLED="true" and ADMIN_SCOPE_CLAIM_VALIDATION_ENABLED="true")
    ADMIN_SCOPE_VALUE: "environment-specific"

    # -- URL for DIAL application
    DIAL_URL: "environment-specific"
    # -- Host for PGVector database
    PGVECTOR_HOST: "environment-specific"
    # -- Port for PGVector database
    PGVECTOR_PORT: "environment-specific"
    # -- Database name for PGVector
    PGVECTOR_DATABASE: "environment-specific"
    # -- Use Azure Managed Identity for PostgreSQL (set to "true" to use MSI instead of PGVECTOR_PASSWORD)
    PGVECTOR_USE_MSI: "false"
    # -- Connection string for Elasticsearch
    ELASTIC_CONNECTION_STRING: "environment-specific"
    # -- Index for Elasticsearch indicators
    ELASTIC_INDICATORS_INDEX: "environment-specific"
    # -- Index for Elasticsearch matching
    ELASTIC_MATCHING_INDEX: "environment-specific"

    ### Data Related Variables ###
    # Configure these variables based on the data uploaded to the application
    # Must be defined for both chat-backend and admin-backend
    # Example:
    # DATA_PORTAL_URL: "example"
    # Note: DATA_PORTAL_API_KEY should be configured in secrets section below

  ### Secrets Configuration ###
  # Sensitive values are stored in Kubernetes Secrets and mounted as environment variables
  # The secrets are automatically created and mounted by the dial-extension chart
  # ref: https://kubernetes.io/docs/concepts/configuration/secret/
  secrets: {}
    # Uncomment and configure the following secrets in your environment-specific values:
    #
    # -- DIAL API key for authentication
    # DIAL_API_KEY: ""
    # -- PGVector database username
    # PGVECTOR_USER: ""
    # -- PGVector database password
    # PGVECTOR_PASSWORD: ""
    # -- Elasticsearch authentication username
    # ELASTIC_AUTH_USER: ""
    # -- Elasticsearch authentication password
    # ELASTIC_AUTH_PASSWORD: ""

    ### Data Related Variables ###
    # -- Example for data related variables
    # DATA_PORTAL_API_KEY: "example"

  ### Ingress Configuration ###
  # ref: https://kubernetes.io/docs/concepts/services-networking/ingress/
  ingress:
    # -- Enable Ingress resource
    enabled: false
    # -- Specify the Ingress class name
    ingressClassName: "nginx"
    # -- Path for the Ingress resource
    path: "/admin/api"
    # -- NGINX annotations for proxy configuration
    annotations:
      nginx.ingress.kubernetes.io/proxy-connect-timeout: "600"
      nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
      nginx.ingress.kubernetes.io/proxy-send-timeout: "600"

admin-frontend:
  # -- Indicates whether the admin-frontend service is enabled
  enabled: false

  commonLabels:
    # -- Kubernetes label to identify the component as an application
    app.kubernetes.io/component: "application"

  ### Container Image Configuration ###
  # ref: https://kubernetes.io/docs/concepts/containers/images/
  image:
    # -- Docker registry URL
    registry: docker.io
    # -- Image repository name
    repository: epam/statgpt-admin-frontend
    # -- Image tag or version
    tag: *admin_frontend_version
    # -- Image pull policy
    pullPolicy: Always

  ### Container Port Configuration ###
  # ref: https://kubernetes.io/docs/concepts/services-networking/service/
  containerPorts:
    # -- HTTP port for the application
    http: 3000

  ### Liveness Probes configuration ###
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  livenessProbe:
    # -- Enable livenessProbe
    enabled: true
    # -- HTTP GET request configuration for liveness probe
    httpGet:
      # -- Health check endpoint path
      path: /api/health

  ### Readiness Probes configuration ###
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  readinessProbe:
    # -- Enable readinessProbe
    enabled: true
    # -- HTTP GET request configuration for liveness probe
    httpGet:
      # -- Health check endpoint path
      path: /api/health

  ### Resource Management ###
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  resources:
    limits:
      # -- Maximum CPU limit for the container
      cpu: "1000m"
      # -- Maximum memory limit for the container
      memory: "4Gi"
    requests:
      # -- Minimum CPU request for resource scheduling
      cpu: "500m"
      # -- Minimum memory request for resource scheduling
      memory: "0.5Gi"

  metrics:
    # -- Enable metrics collection
    enabled: false
    serviceMonitor:
      # -- Enable Prometheus ServiceMonitor for metrics
      enabled: false

  ### Environment Variables ###
  env:
    # -- DIAL API URL
    DIAL_API_URL: "environment-specific"
    # -- API URL for admin-backend
    API_URL: "environment-specific"
    # -- URL for NextAuth service
    NEXTAUTH_URL: "environment-specific"

    ### Keycloak Identity Provider ###
    # -- Client ID for Keycloak
    # AUTH_KEYCLOAK_CLIENT_ID: ""
    # -- URL for Keycloak
    # AUTH_KEYCLOAK_HOST: ""

    ### Azure Identity Provider ###
    # -- Azure AD Client ID for authentication
    # AUTH_AZURE_AD_CLIENT_ID: ""
    # -- Azure AD Application Name
    # AUTH_AZURE_AD_NAME: ""
    # -- Azure Authority URL for authentication
    # AUTH_AZURE_AUTHORITY_URL: ""
    # -- Azure AD Scopes for authentication
    # AUTH_AZURE_AD_SCOPE: ""
    # -- Azure AD Tenant ID
    # AUTH_AZURE_AD_TENANT_ID: ""

  ### Secrets Configuration ###
  # Sensitive values are stored in Kubernetes Secrets and mounted as environment variables
  # The secrets are automatically created and mounted by the dial-extension chart
  # ref: https://kubernetes.io/docs/concepts/configuration/secret/
  secrets: {}
    # Uncomment and configure the following secrets in your environment-specific values:
    #
    # -- DIAL API key for authentication
    # DIAL_API_KEY: ""
    # -- Secret for NextAuth session encryption (can be generated with: openssl rand -base64 64)
    # NEXTAUTH_SECRET: ""
    #
    ### Keycloak Identity Provider ###
    # -- Keycloak client secret
    # AUTH_KEYCLOAK_SECRET: ""
    #
    ### Azure Identity Provider ###
    # -- Azure AD client secret
    # AUTH_AZURE_AD_SECRET: ""

  ### Ingress Configuration ###
  # ref: https://kubernetes.io/docs/concepts/services-networking/ingress/
  ingress:
    # -- Enable Ingress resource
    enabled: false
    # -- Specify the Ingress class name
    ingressClassName: "nginx"
    # -- Path for the Ingress resource
    path: "/"
    # -- NGINX annotations for proxy configuration
    annotations:
      nginx.ingress.kubernetes.io/proxy-connect-timeout: "600"
      nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
      nginx.ingress.kubernetes.io/proxy-send-timeout: "600"

pgvector:
  # -- Indicates whether the pgvector service is enabled
  enabled: false

  ### Container Image Configuration ###
  # ref: https://kubernetes.io/docs/concepts/containers/images/
  image:
    # -- Docker registry URL
    registry: docker.io
    # -- Image repository name
    repository: bitnamilegacy/postgresql
    # -- Image tag or version
    tag: *postgresql_version

  metrics:
    image:
      # -- Fix: override deprecated repository with the updated one
      repository: bitnamilegacy/postgres-exporter

  volumePermissions:
    image:
      # -- Fix: override deprecated repository with the updated one
      repository: bitnamilegacy/os-shell

  auth:
    # -- Database name
    database: "statgpt"
    # -- Custom database username
    username: "statgpt"

    ### Sensitive Credentials ###
    # The following values are automatically converted to Kubernetes Secrets by the Bitnami PostgreSQL chart
    # Chart template: https://github.com/bitnami/charts/blob/postgresql/15.5.5/bitnami/postgresql/templates/secrets.yaml
    # Uncomment and configure in your environment-specific values:
    #
    # -- Password for the "postgres" admin user (stored as "postgres-password" in Secret)
    # postgresPassword: ""
    # -- Password for the custom database user (stored as "password" in Secret)
    # password: ""

  # -- PostgreSQL shared preload libraries including pgaudit for auditing and vector for pgvector extension
  postgresqlSharedPreloadLibraries: "pgaudit,vector"

  # -- PGVector extension version to build and install
  pgvectorVersion: *pgvector_version

  primary:
    resources:
      limits:
        # -- Maximum CPU limit for the container
        cpu: "4000m"
        # -- Maximum memory limit for the container
        memory: "4Gi"
      requests:
        # -- Minimum CPU request for resource scheduling
        cpu: "2000m"
        # -- Minimum memory request for resource scheduling
        memory: "2Gi"

    # -- Build and install pgvector extension by copying existing PostgreSQL directories and adding pgvector files
    initContainers:
      - name: setup-pgvector
        image: "{{ .Values.image.registry }}/{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        command:
          - /bin/bash
          - -c
          - |
            set -e
            echo "=== Setting up PostgreSQL with pgvector extension ==="

            PGVECTOR_VERSION="{{ .Values.pgvectorVersion }}"
            echo "Installing pgvector version: $PGVECTOR_VERSION"

            # Create directory structure in shared volume
            mkdir -p /shared/postgresql-files/lib /shared/postgresql-files/share

            # Copy entire existing PostgreSQL directories to preserve all libraries and extensions
            echo "Copying existing PostgreSQL directories..."
            cp -r /opt/bitnami/postgresql/lib/* /shared/postgresql-files/lib/
            cp -r /opt/bitnami/postgresql/share/* /shared/postgresql-files/share/

            # Install build dependencies for compiling pgvector
            echo "Installing build dependencies..."
            install_packages git build-essential

            # Clone and build pgvector from source
            echo "Building pgvector extension version $PGVECTOR_VERSION..."
            cd /tmp
            git clone --branch "$PGVECTOR_VERSION" https://github.com/pgvector/pgvector.git
            cd pgvector
            export PG_CONFIG=/opt/bitnami/postgresql/bin/pg_config
            make clean && make

            # Add compiled pgvector files to the copied PostgreSQL directories
            echo "Installing pgvector files..."
            cp vector.so /shared/postgresql-files/lib/
            cp sql/vector--*.sql /shared/postgresql-files/share/extension/
            cp vector.control /shared/postgresql-files/share/extension/

            # Set proper ownership for Bitnami PostgreSQL user
            chown -R 1001:1001 /shared

            # Verify installation
            echo "Verifying pgvector installation..."
            ls -la /shared/postgresql-files/lib/vector.so
            ls -la /shared/postgresql-files/share/extension/vector.control

            echo "=== pgvector $PGVECTOR_VERSION setup completed successfully ==="
        volumeMounts:
          - name: postgresql-files
            mountPath: /shared
        securityContext:
          runAsUser: 0
          runAsNonRoot: false

    # -- Shared volume for storing complete PostgreSQL directories with pgvector extension
    extraVolumes:
      - name: postgresql-files
        emptyDir: {}

    # -- Mount complete PostgreSQL directories (lib and share) with pgvector extension included
    extraVolumeMounts:
      - name: postgresql-files
        mountPath: /opt/bitnami/postgresql/lib
        subPath: postgresql-files/lib
      - name: postgresql-files
        mountPath: /opt/bitnami/postgresql/share
        subPath: postgresql-files/share

    # -- Database initialization scripts to create the vector extension after PostgreSQL startup
    initdb:
      scripts:
        01_create_extension.sh: |
          #!/bin/sh
          set -e

          export PGPASSWORD=$POSTGRES_POSTGRES_PASSWORD

          echo "=== Creating the vector extension in the database ==="
          psql -U postgres -d $POSTGRES_DATABASE -c "CREATE EXTENSION IF NOT EXISTS vector;"

          echo "Currently installed extensions:"
          psql -U postgres -d $POSTGRES_DATABASE -c "\dx"

          echo "=== The vector extension has been successfully created in the database ==="

elasticsearch:
  # -- Indicates whether the elasticsearch service is enabled
  enabled: false

  ### Container Image Configuration ###
  # ref: https://kubernetes.io/docs/concepts/containers/images/
  image:
    # -- Docker registry URL
    registry: docker.io
    # -- Image repository name
    repository: bitnamilegacy/elasticsearch
    # -- Image tag or version
    tag: *elasticsearch_version

  metrics:
    image:
      # -- Fix: override deprecated repository with the updated one
      repository: bitnamilegacy/elasticsearch-exporter

  volumePermissions:
    image:
      # -- Fix: override deprecated repository with the updated one
      repository: bitnamilegacy/os-shell

  sysctlImage:
    # -- Fix: override deprecated repository with the updated one
    repository: bitnamilegacy/os-shell

  # -- Indicates whether Kibana is enabled
  kibanaEnabled: false

  master:
    # -- Specifies if master nodes are master-only
    masterOnly: false
    # -- Number of master replicas
    replicaCount: 1
    resources:
      limits:
        # -- Maximum CPU limit for the container
        cpu: "1000m"
        # -- Maximum memory limit for the container
        memory: "1Gi"
      requests:
        # -- Minimum CPU request for resource scheduling
        cpu: "500m"
        # -- Minimum memory request for resource scheduling
        memory: "0.5Gi"
  data:
    # -- Number of data node replicas
    replicaCount: 0
  coordinating:
    # -- Number of coordinating node replicas
    replicaCount: 0
  ingest:
    # -- Number of ingest node replicas
    replicaCount: 0

  security:
    # -- Enable security features
    enabled: true
    # -- Auto-generate TLS certificates
    tls:
      autoGenerated: true

    ### Sensitive Credentials ###
    # The elastic user password is stored in the main Kubernetes Secret
    # Chart template: https://github.com/bitnami/charts/blob/elasticsearch/21.3.2/bitnami/elasticsearch/templates/secrets.yaml
    # Uncomment and configure in your environment-specific values:
    #
    # -- Password for the "elastic" superuser (stored as "elasticsearch-password" in Secret)
    # elasticPassword: ""

portal-frontend:
  # -- Indicates whether the portal-frontend service is enabled
  enabled: false

  commonLabels:
    # -- Kubernetes label to identify the component as an application
    app.kubernetes.io/component: "application"

  ### Container Image Configuration ###
  # ref: https://kubernetes.io/docs/concepts/containers/images/
  image:
    # -- Docker registry URL
    registry: docker.io
    # -- Image repository name
    repository: epam/statgpt-global-trusted-data-commons
    # -- Image tag or version
    tag: *portal-frontend_version
    # -- Image pull policy
    pullPolicy: Always

  ### Container Port Configuration ###
  # ref: https://kubernetes.io/docs/concepts/services-networking/service/
  containerPorts:
    # -- HTTP port for the application
    http: 3000

  ### Liveness Probes configuration ###
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  livenessProbe:
    # -- Enable livenessProbe
    enabled: true
    # -- HTTP GET request configuration for liveness probe
    httpGet:
      # -- Health check endpoint path
      path: /api/health

  ### Readiness Probes configuration ###
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  readinessProbe:
    # -- Enable readinessProbe
    enabled: true
    # -- HTTP GET request configuration for liveness probe
    httpGet:
      # -- Health check endpoint path
      path: /api/health

  ### Resource Management ###
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  resources:
    limits:
      # -- Maximum CPU limit for the container
      cpu: "1000m"
      # -- Maximum memory limit for the container
      memory: "4Gi"
    requests:
      # -- Minimum CPU request for resource scheduling
      cpu: "100m"
      # -- Minimum memory request for resource scheduling
      memory: "0.5Gi"

  metrics:
    # -- Enable metrics collection
    enabled: false
    serviceMonitor:
      # -- Enable Prometheus ServiceMonitor for metrics
      enabled: false

  ### Environment Variables ###
  env:
    # -- DIAL API URL
    DIAL_API_URL: "environment-specific"
    # -- DIAL API Version
    DIAL_API_VERSION: "environment-specific"
    # -- Default model
    DEFAULT_MODEL: "environment-specific"
    # -- SDMX API URL
    SDMX_API_URL: "environment-specific"
    # -- SDMX Constraints API URL (optional)
    # CONSTRAINS_SDMX_API_URL: "environment-specific"
    # -- URL for NextAuth service
    NEXTAUTH_URL: "environment-specific"

    ### Keycloak Identity Provider ###
    # -- Client ID for Keycloak
    # AUTH_KEYCLOAK_CLIENT_ID: ""
    # -- URL for Keycloak
    # AUTH_KEYCLOAK_HOST: ""

  ### Secrets Configuration ###
  # Sensitive values are stored in Kubernetes Secrets and mounted as environment variables
  # The secrets are automatically created and mounted by the dial-extension chart
  # ref: https://kubernetes.io/docs/concepts/configuration/secret/
  secrets: {}
    # Uncomment and configure the following secrets in your environment-specific values:
    #
    # -- DIAL API key for authentication
    # DIAL_API_KEY: ""
    # -- SDMX authentication key (required if SDMX API requires authentication)
    # SDMX_AUTH_KEY: ""
    # -- Secret for NextAuth session encryption (can be generated with: openssl rand -base64 64)
    # NEXTAUTH_SECRET: ""
    #
    ### Keycloak Identity Provider ###
    # -- Keycloak client secret
    # AUTH_KEYCLOAK_SECRET: ""

  ### Ingress Configuration ###
  # ref: https://kubernetes.io/docs/concepts/services-networking/ingress/
  ingress:
    # -- Enable Ingress resource
    enabled: false
    # -- Specify the Ingress class name
    ingressClassName: "nginx"
    # -- Path for the Ingress resource
    path: "/"
    # -- NGINX annotations for proxy configuration
    annotations:
      nginx.ingress.kubernetes.io/proxy-connect-timeout: "600"
      nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
      nginx.ingress.kubernetes.io/proxy-send-timeout: "600"
